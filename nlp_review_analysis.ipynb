{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Categorizing Customer df_reviews**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Import Packages and Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import math\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords as nltk_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set style\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df_reviews = pd.read_csv('/Users/kellyshreeve/Desktop/Data-Sets/imdb_reviews.tsv', \n",
    "                      sep = '\\t', parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data info\n",
    "print(df_reviews.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data head\n",
    "display(df_reviews.sample(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial data observations\n",
    "\n",
    "The dataset contains 47331 observations and 16 columns representing movie df_df_df_reviews and their classification, positive or negative. Runtime_minutes needs to be converted to int64. There are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Prepare Data**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fix data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check values of runtime\n",
    "display(sorted(df_reviews['runtime_minutes'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing values for runtime were entered as '\\\\N', which is causing the data to be mis-typed as object. Changing '\\\\N' to np.nan will allow the variable to be converted to to float64."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace '\\\\N' with np.nan\n",
    "df_reviews['runtime_minutes'] = df_reviews['runtime_minutes'].replace('\\\\N', np.nan)\n",
    "\n",
    "# Convert runtime to int64\n",
    "df_reviews['runtime_minutes'] = df_reviews['runtime_minutes'].astype('float')\n",
    "\n",
    "# Display info\n",
    "print(df_reviews.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Runtime_minutes is converted to float."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for full duplicates\n",
    "duplicates_count = df_reviews.duplicated().sum()\n",
    "\n",
    "print(f'Number of full duplicates: {duplicates_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no fully duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for review duplicates\n",
    "review_duplicates = df_reviews['review'].duplicated().sum()\n",
    "\n",
    "print(f'Number of duplicated review texts: {review_duplicates}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 91 duplicated df_reviews. Print rows to further inspect the duplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print duplicated rows\n",
    "duplicate_review_text = df_reviews['review'].duplicated(keep=False)\n",
    "\n",
    "display(df_reviews[duplicate_review_text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rows are fully duplicated other than the idx index variable. Duplicates will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "reivews = df_reviews.drop_duplicates(subset=['review'], inplace=True)\n",
    "\n",
    "duplicates_new = df_reviews['review'].duplicated().sum()\n",
    "\n",
    "print(f'Updated number of duplicates: {duplicates_new}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates have been removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace all '\\N' with np.nan\n",
    "df_reviews = df_reviews.replace('\\\\N', np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate number and percent of missing values by columns\n",
    "missing = df_reviews.isna().sum().reset_index().rename(columns=\n",
    "                                                       {'index':'column',\n",
    "                                                        0:'count'})\n",
    "\n",
    "missing['percent'] = ((missing['count'] / len(df_reviews)) * 100).round(2)\n",
    "\n",
    "print('Missing Values:')\n",
    "display(missing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "End_year is missing over 95% of values, runtime_minutes is missing 1% of valeus, and genres, average_rating, and votes are all missing less than 1% of values. Because of the high number missing in end_year, this variable will be dropped from the analysis. The number missing in other variables is so low that imputation isn't necessary. The rows missing information will be dropped from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows missing values in runtime, genre, rating, or votes\n",
    "df_reviews = df_reviews.dropna(subset=['runtime_minutes', 'genres', 'average_rating', 'votes'])\n",
    "\n",
    "missing_dropped = df_reviews.isna().sum().reset_index().rename(columns=\n",
    "                                                       {'index':'column',\n",
    "                                                        0:'count'})\n",
    "\n",
    "missing_dropped['percent'] = ((missing_dropped['count'] / len(df_reviews)) * 100).round(2)\n",
    "\n",
    "print('Missing values after dropping missing rows:')\n",
    "display(missing_dropped)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no more missing values, after dropping rows missing in the runtime, genre, rating, and votes subest. End_year will be left out of analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data conclusion\n",
    "\n",
    "Duplicates and missing values have been dropped from the dataset. The data is ready for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Exploratory data analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of unique titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find number of movies reviewed in the dataset\n",
    "movie_count = df_reviews['primary_title'].nunique()\n",
    "\n",
    "print(f'Number of unique movie titles: {movie_count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reviews by media type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph number of movies and review tone by media type\n",
    "fig, axs = plt.subplots(2, 1, figsize=(8, 8))\n",
    "\n",
    "# Media frequency plot\n",
    "type_count = sns.countplot(ax=axs[0], data=df_reviews, y='title_type', color='steelblue')\n",
    "type_count.set(title='Frequency of Media Type', xlabel='Count', ylabel='Media')\n",
    "\n",
    "# Review tone by media plot\n",
    "type_review = sns.countplot(ax=axs[1], data=df_reviews, y='title_type', hue='sp', \n",
    "                            hue_order=['pos', 'neg'], palette=('darkseagreen', 'steelblue'))\n",
    "type_review.set(title='Review Tone by Media Type', xlabel='Count', ylabel='Media')\n",
    "\n",
    "# Show plots\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most reviews are for movies. There are only a small number of reviews for other media types such as shorts, tv episodes, videos, tv movies, etc. Movies, shorts, tv episodes, and tv series tend to have more positive than negative reviews. Videos and tvmovies have more negative than positive reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movies and reviews by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create supblots\n",
    "fig, axs = plt.subplots(2, 1, figsize=(16, 8))\n",
    "\n",
    "# Number of movies by year\n",
    "ax0 = axs[0]\n",
    "\n",
    "movies_year = df_reviews.groupby('start_year')['tconst'].nunique()\n",
    "movies_year = movies_year.reindex(index=np.arange(movies_year.index.min(), \n",
    "                                    max(movies_year.index.max(), 2021))).fillna(0)\n",
    "\n",
    "movies_year.plot(kind='bar', color='steelblue', ax=ax0,\n",
    "                 title='Number of Movies Over Years', xlabel='Year', \n",
    "                 ylabel='Number of Movies', width=0.7)\n",
    "\n",
    "# Tone of review over time\n",
    "ax1 = axs[1]\n",
    "\n",
    "pos_year = df_reviews.groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "pos_year = pos_year.reindex(index=np.arange(pos_year.index.min(), \n",
    "                                            max(pos_year.index.max(), 2023)), fill_value=0)\n",
    "\n",
    "pos_year.plot(kind='bar', ax=ax1, stacked=True, color=['steelblue', 'darkseagreen'],\n",
    "              legend='reverse', width=0.7)\n",
    "\n",
    "plt.title('Tone and Reviews per Movie Over Years')\n",
    "plt.xlabel('Year')\n",
    "plt.ylabel('Number of Reviews')\n",
    "ax1.add_artist(ax1.legend(['Neg', 'Pos'], reverse=True))\n",
    "\n",
    "# Add rolling average reviews per movie over 5 years\n",
    "axt = ax1.twinx() \n",
    "\n",
    "review_total = df_reviews.groupby('start_year')['tconst'].count()\n",
    "review_total = review_total.reindex(index=np.arange(review_total.index.min(), \n",
    "                                                    max(review_total.index.max(), 2023))).fillna(0)\n",
    "review_movie_year = (review_total / movies_year).fillna(0)\n",
    "\n",
    "review_movie_year.reset_index(drop=True).rolling(5).mean() \\\n",
    ".plot(kind='line', ax=axt, color='orange', label='Reviews per Movie (avg over 5 years)')\n",
    "\n",
    "lines, labels = axt.get_legend_handles_labels()\n",
    "ax1.legend(lines, labels, loc='upper left')\n",
    "\n",
    "# Display figures\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of movies per year generally increases over time until 2006, when we see a sharp decline in number of movies produced per year. There are generally similar numbers of positive and negative reviews per year. The number of reviews per movie tends to increase over time, from about 1 review per movie in the early 1900s to amost 10 reviews per movie in 2010."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "review_movie = df_reviews.groupby('tconst')['review'].count() \\\n",
    "    .value_counts().sort_index()\n",
    "\n",
    "# Barplot of review per movie frequency\n",
    "ax0 = axs[0]\n",
    "\n",
    "review_movie.plot(kind='bar', ax=ax0, title='Barplot of Frequencies of Reviews per Movie',\n",
    "                  xlabel='Number of Reviews', ylabel='Number of Movies')\n",
    "\n",
    "# KDE of review per movie frequency\n",
    "ax1 = axs[1]\n",
    "\n",
    "review_movie = df_reviews.groupby('tconst')['review'].count()\n",
    "\n",
    "sns.kdeplot(data=review_movie, ax=ax1)\n",
    "\n",
    "ax1.set_title('KDE of Frequencies of Reviews per Movie')\n",
    "ax1.set_xlabel('Number of Reviews')\n",
    "ax1.set_ylabel('Percent')\n",
    "\n",
    "# Display\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most movies tend to recieve betweeen 1 - 5 reviews per movie. The number of movies tends to decrease as the number of reviews increase, though there is a spike in a number of movies that have recieved 30 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot positive and negative review in train and test\n",
    "ds_group_tone = sns.countplot(data=df_reviews, x='ds_part', hue='sp', hue_order=['pos', 'neg'],\n",
    "              palette=('darkseagreen', 'steelblue'))\n",
    "\n",
    "ds_group_tone.set(title='Review Tone in Train and Test Sets', xlabel='Count', ylabel='Group')\n",
    "\n",
    "plt.legend(title='Review')\n",
    "plt.ylim([0, 15000])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are similar numbers of positive and negative reviews in the training and test sets. Additionally, there are similar numbers of positive reviews and similar numbers of negative reviews across the training and test sets. The classes are mostly balanced, and the trianing and test set are similar to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chart review tone by year for train/test\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 8), gridspec_kw=dict(width_ratios=(2, 1), height_ratios=(1, 1)))\n",
    "\n",
    "# Plot train movies over time\n",
    "ax0 = axs[0][0]\n",
    "\n",
    "df_train_year = df_reviews[df_reviews['ds_part'] == 'train'].groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "df_train_year = df_train_year.reindex(index=np.arange(df_train_year.index.min(), max(df_train_year.index.max(), 2020))).fillna(0)\n",
    "\n",
    "df_train_year.plot(kind='bar', ax=ax0, stacked=True, color=['steelblue', 'darkseagreen'], width=0.7,\n",
    "                   title='Train: Reviews per Year by Polarity', xlabel='Year', ylabel='Number of Movies')\n",
    "\n",
    "ax0.legend(['Neg', 'Pos'], reverse=True)\n",
    "\n",
    "# Train density plot review tone by movie\n",
    "ax1 = axs[0][1]\n",
    "\n",
    "tone_train = df_reviews[df_reviews['ds_part'] == 'train'].groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(tone_train[0], color='steelblue', label='negative', ax=ax1)\n",
    "sns.kdeplot(tone_train[1], color='darkseagreen', label='positive', ax=ax1)\n",
    "\n",
    "ax1.legend(reverse=True)\n",
    "\n",
    "ax1.set_title('Train: Polarity of Reviews by Movie')\n",
    "ax1.set_xlabel('Movie')\n",
    "ax1.set_ylabel('Percent')\n",
    "\n",
    "# Test movies over time\n",
    "ax2 = axs[1][0]\n",
    "\n",
    "df_test_year = df_reviews[df_reviews['ds_part'] == 'test'].groupby(['start_year', 'pos'])['pos'].count().unstack()\n",
    "df_test_year = df_test_year.reindex(index=np.arange(df_test_year.index.min(), max(df_test_year.index.max(), 2020))).fillna(0)\n",
    "\n",
    "df_test_year.plot(kind='bar', ax=ax2, stacked=True, color=['steelblue', 'darkseagreen'], width=0.7,\n",
    "                   title='Test: Reviews per Year by Polarity', xlabel='Year', ylabel='Number of Movies')\n",
    "\n",
    "ax2.legend(['Neg', 'Pos'], reverse=True)\n",
    "\n",
    "# Test review tone by movie\n",
    "ax3 = axs[1][1]\n",
    "\n",
    "tone_test = df_reviews[df_reviews['ds_part'] == 'test'].groupby(['tconst', 'pos'])['pos'].count().unstack()\n",
    "sns.kdeplot(tone_test[0], color='steelblue', label='negative', ax=ax3)\n",
    "sns.kdeplot(tone_test[1], color='darkseagreen', label='positive', ax=ax3)\n",
    "\n",
    "ax3.legend(reverse=True)\n",
    "\n",
    "ax3.set_title('Test: Polarity of Reviews by Movie')\n",
    "ax3.set_xlabel('Movie')\n",
    "ax3.set_ylabel('Percent')\n",
    "\n",
    "# Display\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are similar distributions of movie reviews by year and by polarity in the training and test sets. The sets are similar and can be used to train and test the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory analysis conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Evaluation Procedure**\n",
    "\n",
    "Create evaluation routine which can be used for all models in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, features_train, target_train, features_test, target_test):\n",
    "    '''Displays the F1 score curve, ROC curve, and precision recall curve, and a\n",
    "    data frame of ROC, APS, Accuracy, and F1 score for training and test sets.\n",
    "    \n",
    "    model: fitted classification model\n",
    "    features_train: features of the training set\n",
    "    target_train: target for the training set\n",
    "    features_test: features of the test set\n",
    "    target_test: target for the test set'''\n",
    "    eval_stats = {}\n",
    "    \n",
    "    fig, axs = plt.subplots(1, 3, figsize=(20, 6))\n",
    "    \n",
    "    for type, features, target in (('train', features_train, target_train), ('test', features_test, target_test)):\n",
    "        \n",
    "        eval_stats[type] = {}\n",
    "        \n",
    "        pred_target = model.predict(features)\n",
    "        pred_proba = model.predict_proba(features)[:, 1]\n",
    "        \n",
    "        # F1 scores\n",
    "        f1_thresholds = np.arange(0, 1.01, 0.05)\n",
    "        f1_scores = [metrics.f1_score(target, pred_proba >= threshold) for threshold in f1_thresholds]\n",
    "        \n",
    "        # ROC\n",
    "        fpr, tpr, roc_thresholds = metrics.roc_curve(target, pred_proba)\n",
    "        roc_auc = metrics.roc_auc_score(target, pred_proba)\n",
    "        eval_stats[type]['ROC AUC'] = roc_auc\n",
    "        \n",
    "        # PRC\n",
    "        precision, recall, pr_thresholds = metrics.precision_recall_curve(target, pred_proba)\n",
    "        aps = metrics.average_precision_score(target, pred_proba)\n",
    "        eval_stats[type]['APS'] = aps\n",
    "        \n",
    "        # Plot threshold curves\n",
    "        if type == 'train':\n",
    "            color = 'blue'\n",
    "        else:\n",
    "            color = 'green'\n",
    "            \n",
    "        # F1 score curve\n",
    "        ax0 = axs[0]\n",
    "        max_f1_score_idx = np.argmax(f1_scores)\n",
    "        ax0.plot(f1_thresholds, f1_scores, color=color, \n",
    "                label=f'{type}, max={f1_scores[max_f1_score_idx]:2f} @ {f1_thresholds[max_f1_score_idx]:2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(f1_thresholds - threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'\n",
    "            ax0.plot(f1_thresholds[closest_value_idx], f1_scores[closest_value_idx], \n",
    "                    color=marker_color, marker='X', markersize=7)\n",
    "        ax0.set_xlim([-0.02, 1.02])\n",
    "        ax0.set_ylim([-0.02, 1.02])\n",
    "        ax0.set_xlabel('Threshold')\n",
    "        ax0.set_ylabel('F1')\n",
    "        ax0.legend(loc='lower center')\n",
    "        ax0.set_title('F1 Score')\n",
    "            \n",
    "        # ROC curve\n",
    "        ax1 = axs[1]\n",
    "        ax1.plot(fpr, tpr, color=color, label=f'{type}, ROC AUC={roc_auc:.2f}')\n",
    "        # setting crosses for some thresholds\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(roc_thresholds - threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax1.plot(fpr[closest_value_idx], tpr[closest_value_idx], color=marker_color, \n",
    "                    marker='X', markersize=7)\n",
    "        ax1.plot([0, 1], [0, 1], color='grey', linestyle='--')\n",
    "        ax1.set_xlim([-0.02, 1.02])\n",
    "        ax1.set_ylim([-0.02, 1.02])\n",
    "        ax1.set_xlabel('FPR')\n",
    "        ax1.set_ylabel('TPR')\n",
    "        ax1.legend(loc='lower center')\n",
    "        ax1.set_title('ROC Curve')\n",
    "\n",
    "\n",
    "        # PRC\n",
    "        ax2 = axs[2]\n",
    "        ax2.plot(recall, precision, color=color, label=f'{type}, AP={aps:.2f}')\n",
    "        for threshold in (0.2, 0.4, 0.5, 0.6, 0.8):\n",
    "            closest_value_idx = np.argmin(np.abs(pr_thresholds-threshold))\n",
    "            marker_color = 'orange' if threshold != 0.5 else 'red'            \n",
    "            ax2.plot(recall[closest_value_idx], precision[closest_value_idx], color=marker_color, \n",
    "                    marker='X', markersize=7)\n",
    "        ax2.set_xlim([-0.02, 1.02])    \n",
    "        ax2.set_ylim([-0.02, 1.02])\n",
    "        ax2.set_xlabel('recall')\n",
    "        ax2.set_ylabel('precision')\n",
    "        ax2.legend(loc='lower center')\n",
    "        ax2.set_title(f'PRC') \n",
    "        \n",
    "        eval_stats[type]['Accuracy'] = metrics.accuracy_score(target, pred_target)\n",
    "        eval_stats[type]['F1'] = metrics.f1_score(target, pred_target)\n",
    "        \n",
    "    df_eval_stats = pd.DataFrame(eval_stats)\n",
    "    df_eval_stats = df_eval_stats.round(2)\n",
    "    df_eval_stats = df_eval_stats.reindex(index=('Accuracy', 'F1', 'APS', 'ROC AUC'))\n",
    "\n",
    "    print(df_eval_stats)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Normalization**\n",
    "\n",
    "Remove special characters and nunbers from text and convert to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to normalize text\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^a-zA-Z']\", ' ', text)\n",
    "    text = text.split()\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply normalize function to review column\n",
    "df_reviews['reviews_norm'] = df_reviews['review'].apply(normalize_text)\n",
    "\n",
    "display(df_reviews[['review', 'reviews_norm']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews are converted to lowercase with special characters removed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train/Validate/Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define train and test subsets\n",
    "df_train = df_reviews[df_reviews['ds_part'] == 'train']\n",
    "df_test = df_reviews[df_reviews['ds_part'] == 'test']\n",
    "\n",
    "# Split train into train and validate\n",
    "df_train, df_validate = train_test_split(df_train, test_size=0.3, random_state=123)\n",
    "\n",
    "# Define train and test targets\n",
    "targ_train = df_train['pos']\n",
    "targ_valid = df_validate['pos']\n",
    "targ_test = df_test['pos']\n",
    "\n",
    "print(f'Train Shape: {df_train.shape}')\n",
    "print(f'Validate Shape: {df_validate.shape}')\n",
    "print(f'Test Shape: {df_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and test set are split approximately 50/50 split. The test set has 270 fewer observations than the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **NLP Classification Models**\n",
    "\n",
    "Train models to classify the tone of the review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 0 - Constant Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit dummy model\n",
    "dummy = DummyClassifier().fit(df_train['reviews_norm'], targ_train)\n",
    "\n",
    "# Evalate train and test set\n",
    "evaluate_model(dummy, df_train['reviews_norm'], targ_train, df_validate['reviews_norm'], targ_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1 - NLTK, TF-IDF and Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatize reviews with nltk lemmatizer\n",
    "nltk_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def nltk_lemmatize(text):\n",
    "    nltk_tokens = word_tokenize(text)\n",
    "    nltk_lemmas = [nltk_lemmatizer.lemmatize(token) for token in nltk_tokens]\n",
    "    nltk_lemmas = \" \".join(nltk_lemmas)\n",
    "    return nltk_lemmas\n",
    "\n",
    "df_train['reviews_nltk_lemma'] = df_train['reviews_norm'].apply(nltk_lemmatize)\n",
    "df_validate['reviews_nltk_lemma'] = df_validate['reviews_norm'].apply(nltk_lemmatize)\n",
    "\n",
    "display(df_train.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reviews are lemmatized via the nltk lemmatizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create TF-IDF Features\n",
    "stop_words = nltk_stopwords.words('english')\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "tf_idf_train = tfidf_vectorizer.fit_transform(df_train['reviews_nltk_lemma'])\n",
    "tf_idf_valid = tfidf_vectorizer.transform(df_validate['reviews_nltk_lemma'])\n",
    "\n",
    "print(f'TF-IDF train matrix size: {tf_idf_train.shape}')\n",
    "print(f'TF-IDF validate matrix size: {tf_idf_valid.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TF-IDF matrix is fit and transformed on the train set and the validate set is transformed. Train and validate TF-IDF matrices both have 55134 columns, representing 55134 words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
